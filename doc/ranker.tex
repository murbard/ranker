\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage[
backend=biber,
style=alphabetic,
sorting=ynt
]{biblatex}

\bibliography{ranker}
\addbibresource{citation-aos32_384.bib}


\begin{document}
\title{Ranker}
\author{Arthur Breitman}
\date{October 2022}
\maketitle
\begin{abstract}
    Ranker is a Bayesian algorithm for ranking items in a set, inspired by the ELO rating
    system in chess. It assumes each item has a latent score, normally distributed
    with a latent variance, and that the outcome of a pairwise comparison is a Bernoulli
    trial with a probability of success given by the logistic function of the difference.

    Ranker infers the latent scores of the items and the variance using a variational
    Bayes approximation which models the posterior as an inverse gamma distributed variance
    and independent normally distributed scores. We compute the KL-divergence gradient and
    Hessian in closed-form by approximating the logistic function with the error function.
    This yields a very fast inference technique, based on the Levenberg-Marquardt algorithm.

    Ranker can do more than infer scores and rank items, it can also be used to select the
    most useful pair of items to compare next. We do this efficiently by computing the
    gradient of our loss function with respect to the observed trials. The entire algorithm
    runs in $\mathcal{O}(n^2)$ time, where $n$ is the number of items.
\end{abstract}

\section{Introduction}
    Ranker is a Bayesian algorithm for ranking items in a set, inspired by the ELO rating
    system in chess. It assumes each item has a latent score, normally distributed
    with a latent variance, and that the outcome of a pairwise comparison is a Bernoulli
    trial with a probability of success given by the logistic function of the difference.

    Ranker infers the latent scores of the items and the variance using a variational
    Bayes approximation which models the posterior as an inverse gamma distributed variance
    and independent normally distributed scores. We compute the KL-divergence gradient and
    Hessian in closed-form by approximating the logistic function with the error function.
    This yields a very fast inference technique, based on the Levenberg-Marquardt algorithm.

    Ranker can do more than infer scores and rank items, it can also be used to select the
    most useful pair of items to compare next. We do this efficiently by computing the
    gradient of our loss function with respect to the observed trials. The entire algorithm
    runs in $\mathcal{O}(n^2)$ time, where $n$ is the number of items.

\section{Background}
    Ranker is inspired by the ELO rating system in chess. In chess, each player has a rating
    which is a positive integer. The rating of a player is a measure of their skill,
    and it is updated after each game. Each player's rating is updated according to the
    following formula:
    \begin{equation}
        R_{new} = R_{old} + K(S - E)
    \end{equation}
    where $R_{new}$ and $R_{old}$ are the new and old ratings, $K$ is a constant which
    determines how much the rating changes, $S$ is the score of the player in the game, and
    $E$ is the expected score of the player, given the ratings of the two players. The expected
    score is computed as follows:
    \begin{equation}
        E = \frac{1}{1 + 10^{-\frac{R_{other} - R_{self}}{400}}}
    \end{equation}
    where $R_{other}$ is the rating of the opponent, and $R_{self}$ is the rating of the player.

    This is an approximate update rule which is only valid when ratings are close to each other.
    It corresponds to a gradient descent algorithm with a learning rate of $K$. The lack of decay
    in the learning rate is a two-edged sword: it allows the ratings to change quickly, reflecting
    a player's recent performance, but it also means that the ratings can change wildly. For that
    reason, $K$ is typically set at $16$ for masters and $32$ for grandmasters.

    A partial Bayesian approach to the ELO rating system is described in BayesElo, using
    Maximum a Posteriori (MAP) estimation, following the work of \cite{10.1214/aos/1079120141}.

\section{Approach}

\subsection{Model}
    Ranker assumes that each item has a latent score, normally distributed with a latent
    variance. The outcome of a pairwise comparison is a Bernoulli trial with a probability
    of success given by the logistic function of the difference. The model is as follows:
    \begin{equation}
        \begin{aligned}
            \nu &\sim \text{InvGamma}(\alpha_h, \beta_h) \\
            \forall i, z_i &\sim \mathcal{N}(0, \nu) \\
            \forall i, j, \text{o}_{i,j} &\sim \text{Binomial}\left(\frac{1}{1 + e^{-(z_i - z_j)}}, m_{i,j}\right)
        \end{aligned}
    \end{equation}
    where $\alpha_h$ and $\beta_h$ are hyperparameters, and $\text{o}_{i,j}$ is the outcome of $m_{i,j}$ comparisons
    between items $i$ and $j$, where $m_{i,j}$ is fixed and given. Note that we use the convention that $\beta_h$ represents
    a rate, not a scale. The scale is $1 / \beta_h$.

    \subsubsection{Choice of hyperparameters}
    We pick concrete values for the hyperparameters $\alpha_h = 1.2$ and $\beta_h = 2$
    A handwavy justification for these values follows.

    Consider the binomial distribution over 10 stars, obtained by flipping a coin
    ten times and summing the number of heads. It has a standard deviation of
    $\sqrt{5/2} \simeq 1.5811$. The difference between two adjacent start ratings is thus
    $\sqrt{2/5} \simeq 0.6325$. Based on this intuition, we call a difference of $\sqrt{2/5}$ standard deviations
    a "star" of difference.

    Assuming this is an intuitive notion of a "star", what odds
    should an additional star confer to an item in a heads up match?
    If those comparisons are hard to judge and very subjective, maybe only 55\% of the time?
    If they're very clear, perhaps over 95\% of the time?

    Using a logistic rule:
    \begin{itemize}
    \item 55\% would correspond to a standard deviation of $\sqrt{5/2} \log(11/9) \simeq 0.317$
    \item 99\% would correspond to a standard deviation of $\sqrt{5/2} \log(19) \simeq 4.656$
    \end{itemize}

    None of this is very rigorous, but it gives use some sense of the scale
    of standard deviations we're dealing with. Clearly 0.0001 and 100 are silly.

    These roughly correspond to the 1\% and 99\% tails of the
    $\text{InverseGamma}(1.2, 2)$ distribution for the variance and justifies our choice of hyperparameters.
    We are proud to incorporate actual prior knowledge, as opposed
    to coping out of it by using a so-called ``uninformative" prior :-)

\subsection{Variational Bayes approximation}

We approximate the posterior with a factorized distribution.

\begin{equation}
    \begin{aligned}
        \nu &\sim \text{InvGamma}(\alpha, \beta) \\
        \forall i, z_i &\sim \mathcal{N}(\mu_i, \sigma_i) \\
    \end{aligned}
\end{equation}

Note that $\alpha$ and $\beta$ are parameters of the approximation to the posterior, not the hyperparameters of the model, which are
$\alpha_h$ and $\beta_h$. Nonetheless, we naturally initialize $\alpha$ and $\beta$ to $\alpha_h$ and $\beta_h$. We initialize $\mu_i$ to $0$.
The case of $\sigma_i$ is more complicated. Absent any observation, the distribution of $z_i$ is a Student's distribution with
infinite variance. We choose to initialize $\sigma_i$ to minimize the KL divergence with that Student distribution. We discovered
empirically (but did not attempt to prove) that the minimum is reached for
$$\sigma_i = 1 + \frac{3}{6 \alpha_h + 2 \alpha_h^2 + \mathcal{O}(\alpha_h^3)}$$


\section{Closed form expression }

\subsection{KL-divergence}

The KL divergence between the true posterior, $P$, and the variational approximation $Q$ is thus $D_{KL}(Q||P) = - H(Q) + H(Q,P)$,
where $H(Q)$ is the entropy of $Q$ and $H(Q,P)$ is the cross entropy of $Q$ and $P$. The entropy of $Q$ is

\begin{multline*}
    H(Q) = \int_{\nu=0}^{\infty} \frac{\beta^{-\alpha}\nu^{-\alpha-1}e^{-\frac{1}{\beta \nu}}}{\Gamma(\alpha) }
    \left(\alpha_h \log(\beta_h) + (\alpha+1)\log(\nu) + \frac{1}{\beta \nu} + \log(\Gamma(\alpha))\right)
    d\nu \\
    + \sum_{i=1}^n \int_{z_i=-\infty}^{\infty} \frac{e^{-\frac{1}{2}\left(\frac{z_i-\mu_i}{\sigma_i}\right)^2}}{\sigma_i \sqrt{2 \pi}}
    \left(\frac{1}{2} \log{2\pi} + \log{\sigma_i} + \frac{1}{2}\left(\frac{z_i-\mu_i}{\sigma_i}\right)^2\right) dz_i
\end{multline*}


The cross entropy of $Q$ and $P$ is

\begin{multline*}
    H(Q,P) = \int_{\nu=0}^{\infty} \frac{\beta^{-\alpha}\nu^{-\alpha-1}e^{-\frac{1}{\beta \nu}}}{\Gamma(\alpha)}
\left(\alpha_h \log(\beta_h) + (\alpha+1)\log(\nu) + \frac{1}{\beta \nu} + \log(\Gamma(\alpha))\right)
\\
+ \sum_{i=1}^n \int_{\nu=0}^{\infty} \int_{z_i=-\infty}^{\infty} \frac{e^{-\frac{1}{2}\left(\frac{z_i-\mu_i}{\sigma_i}\right)^2}}{\sigma_i \sqrt{2 \pi}} \left(
\frac{1}{2} \log{2\pi} + \log(\nu) + \frac{1}{2}\frac{z_i^2}{\nu}
\right)dz_i d\nu\\
+ \sum_{i=1}^n\sum_{j=1}^n m_{i,j} \int_{z_\delta=-\infty}^{\infty}
\frac{e^{-\frac{1}{2}\left(\frac{z_\delta - (\mu_i - \mu_j)}{\sqrt{\sigma_i^2 + \sigma_j^2}}\right)^2}}{\sqrt{\sigma_i^2 + \sigma_j^2} \sqrt{2 \pi}} \log\left(1 + e^{-z_\delta}\right) z_\delta +
Cst
\end{multline*}


\subsection{The logistic-normal integral}

As it turns out, we can express most of the integrals above in terms of elementary functions, as well as the polygamma function $\Psi$.
The integral

$$\int_{z_\delta=-\infty}^{\infty}
\frac{e^{-\frac{1}{2}\left(\frac{z_\delta - (\mu_i - \mu_j)}{\sqrt{\sigma_i^2 + \sigma_j^2}}\right)^2}}{\sqrt{\sigma_i^2 + \sigma_j^2} \sqrt{2 \pi}} \log\left(1 + e^{-z_\delta}\right) dz_\delta$$

is more problematic. A related integral is the logistic-normal integral,

$$\int_{z_\delta=-\infty}^{\infty}
\frac{e^{-\frac{1}{2}\left(\frac{z_\delta - (\mu_i - \mu_j)}{\sqrt{\sigma_i^2 + \sigma_j^2}}\right)^2}}{\sqrt{\sigma_i^2 + \sigma_j^2} \sqrt{2 \pi}} \frac{1}{1 + e^{-z_\delta)}}$$

\cite{crooks2009logistic} offers an approximation by replacing the logistic function with an error function:

$$\frac{1}{1+e^{-z}} \simeq \frac{1}{2} + \frac{1}{2} \text{erf}\left(\frac{\sqrt{\pi}}{4} z\right)$$

Given that $\frac{d}{d z}\log\left(1 + e^{-z}\right) = 1 - \frac{1}{1 + e^{-z}}$ we choose to approximate $\log\left(1 + e^{-z}\right)$
as

$$\log\left(1 + e^{-z}\right) \simeq \int_{\zeta=z}^{\infty} \left(\frac{1}{2} - \frac{1}{2} \text{erf}\left(\frac{\sqrt{\pi}}{4} \zeta \right)\right)~d\zeta =
\frac{2}{\pi}e^{-\frac{\pi z^2}{16}} - \frac{1}{2} z~\mathrm{erfc}\left(\frac{\sqrt{\pi}}{4} z\right)$$

The difference between the two is maximal for $z = 0$ and is $\log(2) - \frac{2}{\pi} \simeq 0.0565$. The difference between the derivatives with respect to $z$ is at most $0.0177$.



\subsection{Gradient and Hessian}










\printbibliography

\end{document}
